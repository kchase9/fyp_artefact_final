# Kristeen Chase: 1039656 - Artefact

Included in this repository are the necessary files that represent the artefact for Kristeen Chase's Computer Science thesis: Creole Representation in Automated Speech Recognition: Benchmarking the Performance of Speech Recognition Architectures.

*At the time of submission, the original trained files cannot be uploaded to Github. As such, the versions stored in this repository were cleaned and commented for viewing. To view the trained files, visit: https://github.com/kchase9/fyp_repository.git*

## Abstract
Guyanese Creole (Creolese) is a low resource language that is commonly spoken by locals of Guyana. Most Automated Speech Recognition (ASR) models lack the ability to accurately transcribe the language due to the lack of technical support and resources. In this research project, we create a custom dataset of two hours of Creolese audio, transcribed using an academically-accepted, pronunciation-based spelling system. This dataset was used in the process of identifying the best model for Creolese speech-to-text. Three popular models were selected, representing the Attention-based Encoder Decoder (AED), Connectionist Temporal Classification (CTC) and Recurrent Neural Network Transducer (RNN-T) architectures of ASR models. The dataset was used to fine tune two of these models, and the performance of all five implementations were compared to identify the most suitable one. Through this process, we identified Whisper, the representative of the AED architecture, as the best performing model, despite being prone to oscillation errors. Using this model, we created an interface for Creolese Speech-to-Text to demonstrate a proof of concept to aid in discussions about the successes and shortcomings of this process. Lastly, we identify avenues of future research that would facilitate the integration of Creolese into the field of ASR.

## Content
This repository contains the files that were used to build and fine tune three models: Wav2Vec2 2.0, Whisper, and NVIDIA's Multilingual FastConformer.
In the folder named "models", you will find the Jupyter Notebooks used to host those three models. To run them, open the command line and run:

```bash
jupyter notebook
```
This will generate a browser interface that allow you to view and run those files. Each file contains code blocks that are intended to be run sequentially.

The folders labelled "tokenizer_files", "training_outputs" "trained_predictions" and "pretrained_predictions" will hold the files generated by running the programs 

## How to reproduce the environment:
In the root folder, open the Command Prompt and perform the following tasks:

### Create necessary folders
In the command prompt, run the following commands:

Linux:
```bash
mkdir -p tokenizer_files training_outputs/wav2vec2_training_output training_outputs/whisper_training_output trained_predictions pretrained_predictions
```

Windows:

``` bash
mkdir tokenizer_files training_outputs\wav2vec2_training_output training_outputs\whisper_training_output trained_predictions pretrained_predictions
```

### Clone the dataset
In the Command Prompt, run the following line:

```bash
git clone https://github.com/kchase9/creolese-audio-dataset.git
```

This will download and store the custom dataset that was created to fine tune these models.

### Install requirements
In the Command Prompt, run the following line:

```bash
pip install -r requirements.txt
```

This will install the dependencies required to run the models. 

---
